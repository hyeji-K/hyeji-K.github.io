---
layout: post
# title: [우당탕탕 프로젝트] 1.OCR 기능 이용하기!
date: 2024-12-22
categories: geultto
tags: [geultto, project]
---

한 달간 OCR과 번역 모델을 이용한 프로젝트를 진행하게 되었다. 본 프로젝트로 들어가기 전 가볍게 체험판(?) 정도의 느낌만 가지고 프로젝트에 참여하려고 했는데, 기획을 하고 계획을 짜보니 생각만큼 내 공부를 병행하면서 가볍게 진행할 수 있는 프로젝트가 아니었다. 인공지능 수업을 들었지만, 이걸 직접 적용해보려고 하니 생각보다 공부할 것들이 더 많았고, 뭔가 개발을 할 때와는 달리 시도를 해보는 것조차 어렵게 느껴졌다. 물론 미리 지레짐작 겁 먹은 것일 수도 있겠지만! 

아무튼 OCR 기능을 필수로 사용해야 했고, 목표는 자연어처리를 위한 모델이 목적이었기에 OCR에 대해서 따로 전처리를 할 필요는 없다고 했다. 그래서 어떤 API를 사용해야할 지를 정해야했다.

일단 사용할 여러 OCR들을 찾아보았고, 파이썬 라이브러리인 EasyOCR과 Apple에서 제공하는 VisionKit을 이용한 OCR, 그리고 멀티모달이 가능한 Gemini API를 테스트 해보기로 했다. 

팀의 목표는 OCR을 이용하여 간판의 글자를 잘 읽어올 수 있게 하는 것이었고, 처음으로 간단한 코드 입력만으로 사용해 볼 수 있는 EasyOCR을 테스트해보았다.

```python
import easyocr
from easyocr import Reader

reader = easyocr.Reader(['ko', 'en'])
result = reader.readtext('/content/drive/MyDrive/images/0084aefa6285b.jpg')

for detection in result:
    print(detection)

"""    
([[456, 198], [924, 198], [924, 326], [456, 326]], '바른식담', 0.8015797734260559)
([[505, 319], [871, 319], [871, 383], [505, 383]], '고기전문 만식담', 0.9961230505938655)
"""
```


여러 번의 테스트를 통해 EasyOCR은 이미지에 있는 간판의 글자를 잘 읽어내지 못하는 것을 확인하였다. 

그 다음으로 Apple에서 제공하는 OCR 프레임워크를 사용하여 테스트 해보기로 했는데, 그 이유는 구현할 서비스가 앱의 형태로 구현할 것이기 때문에 제공해주는 프레임워크를 활용할 수 있다면 좋을 것이라고 생각했다.

```swift
func visionRequest(image: UIImage) {
    guard let cgImage = image.cgImage else { return }
    
    let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
    let request = VNRecognizeTextRequest { request, error in
        guard let observations = request.results as? [VNRecognizedTextObservation], error == nil else { return }
        
        let text = observations.compactMap({
            $0.topCandidates(1).first?.string
        }).joined(separator: "\n")
        print("Vision OCR - \(text)")
    }
    
    let revision3 = VNRecognizeTextRequestRevision3
    request.revision = revision3
    request.recognitionLevel = .accurate
    request.recognitionLanguages = ["ko-KR"]
    request.usesLanguageCorrection = true
        
    do {
        try handler.perform([request])
    } catch {
        print(error)
    }
}

/*
Vision OCR - 6은 민물매운탕
T.753-5294. 754-5294
이네
*/
```

그러나 딱 간판에 상호만 있는 이미지일 때는 잘 읽어오지만, 간판에 상호명 외에 다른 텍스트가 많을 때는 이미지에 있는 모든 텍스트를 읽어오며, 세로 간판일 경우에는 글자를 읽어오는 순서가 뒤죽박죽이 될 때가 있었다. 

마지막으로 멀티 모달인 Gemini 1.5 Flask을 사용하여 테스트를 진행했다. 

```swift
func generateText(image: UIImage) {
    Task {
        let model = GenerativeModel(name: "gemini-1.5-flash-latest", apiKey: APIKey.default)
        
        let prompt = "이미지에서 제일 크게 보이는 텍스트만 추출해주고, 따로 덧붙이는 말은 하지마."
        
        let response = try await model.generateContent(prompt, image)
        if let text = response.text {
            print("Gemini OCR - \(text)")
            
            DispatchQueue.main.async {
                self.homeView.infoLabel.text = "상호명 : \(text)"
            }
        }
    }
}

/*
Gemini OCR - 민들매농장
*/
```

제미나이 API를 사용할 때 좋은 점은 프롬프트를 줘서 원하는 텍스트만 불러오게 할 수 있다는 점이었다. 다른 OCR 처럼 텍스트를 잘 읽어오지 못하는 경우도 있었지만, 테스트한 API 중에서 제일 정확도가 높았다. 핸드폰에 받아서 밖에서 테스트 했을 때에도 밤이었지만, 텍스트를 정확하게 읽어오는 것을 확인할 수 있었고, 이로써 제미나이 API를 이용해서 OCR을 구현하기로 정했다. 그리고 또 더욱 업데이트 된 Gemini 2.0이 나와서 이를 통해서 다시 테스트 해보는 것이 기대된다. 

예전에 프로젝트를 진행할 때 영수증의 텍스트를 입력하는 기능을 구현해야 했었는데 영수증의 모든 텍스트를 입력하는 것이 사용자에게 부담이 될 것이라 생각해서 OCR 기술을 써보면 어떨까 싶어서 찾아본 적이 있었다. 결국에는 사용해보지 못했지만 이번의 기회로 OCR 기능을 써볼 수 있어서 재미있었다.